
Interactive Text Graph Mining with a Prolog-based Dialog Engine.

Paul Tarau and Eduardo Blanco.

   Department of Computer Science and Engineering.

   University of North Texas.

   paul.tarau@unt.edu, eduardo.blanco@unt.edu.

On top of a neural network-based dependency parser and a graph-based natural language processing  module we design a Prolog-based dialog engine that explores interactively a ranked fact database extracted from a text document.

We reorganize dependency graphs to focus on the most relevant content elements of a sentence, integrate sentence identifiers as graph nodes and after ranking the graph we take advantage of the implicit semantic information that dependency links  bring in the form of subject-verb-object, "is-a" and "part-of" relations.

Working on the Prolog facts and their inferred consequences, the dialog engine specializes the text graph with respect to a query and reveals interactively the document's most relevant content elements. 


The open-source code of the integrated system is available at https://github.com/ptarau/DeepRank.

Keywords: 
logic-based dialog engine,
graph-based natural language processing,
dependency graphs,
query-driven salient sentence extraction,
synergies between neural and symbolic text processing.


Introduction
Logic programming languages have been used successfully for inference and planning tasks on restricted domain natural language processing tasks 
 but not much on open domain, large scale information retrieval and knowledge representation tasks. On the other hand, deep learning systems are very good at basic tasks ranging from parsing to factoid-trained question answering systems, but still taking baby steps when emulating human-level inference processing on complex documents. Thus, a significant gap persists between neural and symbolic processing in the field.

A motivation of our work is to help fill this gap by exploring  synergies between the neural, graph based and symbolic ecosystems in solving a practical problem: building a dialog agent, that, after digesting the content of a text document (e.g., a story, a textbook, a scientific paper, a legal document), enables the user to interact with its most relevant content.

We will start with a quick overview of the tools and techniques needed.
Building a state-of-the art Natural Language Processing system requires interaction with multi-paradigm components as emulating their functionality from scratch could easily become a 100-person/years project. In our case, this means integrating a declarative language module, focusing on high level text mining, into the Python-based nltk ecosystem, while relying on the Java-based Stanford CoreNLP toolkit for basic tasks like segmentation, part-of-speech tagging and parsing.

Overview of the System Architecture

Fig.  summarizes the architecture of our system. The Stanford parser is started as a separate server process to which the Python text processing module connects as a client. It interfaces with the Prolog-based dialog engine by generating a clausal representation of the document's structure and content, as well as the user's queries. The dialog engine is responsible for handling the user's queries for which answers are sent back to the Python front-end which handles also the call to OS-level spoken-language services, when activated.

deepsysSystem Architecture0.34deepsys


Today's dependency parsers,
among which the neurally-trained Stanford dependency parser stands out, 
produce highly accurate dependency graphs and part of speech tagged vertices.
 Seen as edges in a text graph, they provide, by contrast to collocations in a sliding window,  "distilled" building blocks through which
 a graph-based natural language processing system can absorb
 higher level linguistic information.
 
Inspired by the effectiveness of algorithms like Google's PageRank, recursive ranking algorithms applied to text graphs have enabled extraction of keyphrases, summaries and relations. Their popularity continues to increase due to the holistic view they shed on the interconnections between text units  that act as recommenders for the most relevant ones, as well as the comparative simplicity of the algorithms. 
At close to 3000 citations and a follow-up of some almost equally as highly cited papers like the TextRank algorithm and its creative descendants have  extended their  applications to a wide variety of document types and social media interactions in a few dozen languages. 

While part of the family of the TextRank descendants, our graph based text processing algorithm  will use information derived from the dependency graphs associated to sentences. With help from the  labels marking the edges of a dependency graph and the part of speech tags associated to its nodes, we will extract rank-ordered facts corresponding to  content elements present in sentences.  We pass these to logic programs that can query them and  infer new relations, beyond those that can be mined directly from the text.

Like in the case of a good search engine,  interaction with a text document will focus on the most relevant 
and semantically coherent elements matching a query. 
With this in mind, the natural feel of an answer syntactically
appropriate for a query is less important than the usefulness of the content elements extracted: just sentences of the document, in their natural order.

We will also enable spoken interaction with the dialog engine, opening doors for the use of the system via voice-based appliances. Applications range from assistive technologies to visually challenged people, live user manuals, teaching from K-12 to graduate level and interactive information retrieval from complex technical or legal documents.




The  most significant contributions of the research work covered by the paper are:
a logic relation post-processor supporting realtime interactive queries 
about a document's content
integration of our algorithms into an open-source system with practical uses helping a reader of a scientific document to interactively familiarize herself with its content

The paper is organized as follows.
Section  describes the graph-based Natural Language Processing module.
Section  describes our Prolog-based dialog engine.
Section  puts in context the main ideas of the paper and justifies some of the architecture choices we have made.
Section  overviews related work and background information.
Section  concludes the paper.




The  graph-based Natural Language Processing moduleWe have organized our Python-based textgraph processing algorithm together with the Prolog-based dialog engine
into a  unified system(Our implementation is available at https://github.com/ptarau/DeepRank.).
We start with the building and the ranking of the text graph. Then, we overview the summary, keyphrase and relation extraction and the creation of the Prolog database that constitutes the logical model of the document,
to be processed by the dialog engine.

Building and ranking the text graph

We connect as a Python client to the Stanford CoreNLP server and use it to provide
our dependency links via the wrapper at https://www.nltk.org/ of the Stanford CoreNLP toolkit. 

Unlike the original TextRank and related approaches that develop special techniques for each text processing task, we  design a unified algorithm to obtain graph representations of documents, that are suitable for keyphrase extraction, summarization and interactive content exploration.

We use unique sentence identifiers and unique lemmas(A lemma is a canonical representation of a word, as it stands in a dictionary, for all its inflections e.g., it is ''be" for "is", "are", "was" etc.)
as nodes of the text graph. As  keyphrases are centered around nouns and good summary sentences are likely to talk about important concepts,  we will need to reverse some links in the dependency graph provided by the parser, to prioritize nouns
and deprioritize verbs, especially auxiliary and modal ones. 
Thus, we redirect the dependency edges toward nouns with subject and object roles, as shown for a simple short sentence  in Fig. as "about" edges.
 
depgraphDependency graph of a simple sentence with redirected  and  newly added arrows 0.24depgraph.pdf

We also create "recommend" links  from words to the sentence identifiers and back from sentences to verbs with predicate roles
to indirectly ensure that sentences recommend and are 
recommended by their content. Specifically, we ensure that sentences 
recommend verbs with predicate function
from where their recommendation spreads to nouns relevant
as predicate arguments (e.g., having subject or object roles).
  
By using the PageRank implementation of the networkx 
toolkit(https://networkx.github.io/),
after ranking the sentence and word nodes of the text graph,
the system is also able to display
subgraphs filtered to contain only the highest ranked nodes, using Python's 
 graphviz library.
An example of text graph, filtered to only show word-to-word links, derived from the U.S. Constitution
(Available as a text document at: https://www.usconstitution.net/const.txt), 
is shown in Fig. .  
constitText graph of the highest ranked words in the U.S. Constitution0.60constit  
  
Pre- and post-ranking graph refinements

The algorithm induces a form of automatic stopword filtering, 
due to the fact that our dependency link arrangement ensures that modifiers with lesser semantic value relinquish their rank by pointing to more significant lexical components.
This is a valid  alternative to explicit "leaf trimming"  before ranking, which remains an option for reducing graph size for large texts or multi-document collections as well as helping with a more focussed relation extraction from the reduced graphs.

Besides word-to-word links, our text graphs connect sentences as additional dependency graph nodes, resulting in a unified keyphrase and summary extraction framework. Note also that, as an option that is relevant
especially for scientific, medical or legal documents, we add firstin links from a word 
to the sentence containing its first occurrence, to prioritize sentences where concepts 
are likely to be defined or explained.

Our reliance on graphs provided by dependency parsers builds a bridge between deep neural network-based machine learning and graph-based natural language processing enabling us to often capture  implicit semantic information.



Summary and keyword extraction

As link configurations  tend to favor very long sentences, a post-ranking normalization is applied for sentence ranking. 

After ordering sentences by rank we extract the highest ranked ones and reorder them in their natural order in the text to form a more coherent summary.

We use the parser's compound phrase tags to  fuse along dependency links. 

We   design our keyphrase synthesis algorithm to ensure that highly ranked  words will pull out their contexts from sentences, to make up meaningful keyphrases. As a heuristic, we mine for a context of 2-4 dependency linked words of a highly ranked noun, while ensuring that the context itself has a high-enough rank, as we compute a weighted average favoring the noun
over the elements of its context.

Relation extraction

We  add subject-verb-object facts extracted from the highest ranked dependency links, enhanced with "is-a" and "part-of" relations using WordNet via the nltk toolkit. We plan in the future to also generate relations from conditional statements  identified following dependency links and involving negations, modalities, conjuncts and disjuncts, to be represented as Prolog rules. 
Subject-verb-object (SVO) relations are extracted directly from the dependency graph and an extra argument is added to the triplet marking the number of the sentence they originate from.

"Is-a"  relations are extracted using WordNet hypernyms 
and 
hyponyms(More general and, respectively, more specific concepts.). 
Similarly, "partof" relations are extracted using 
meronyms and 
holonyms(Concepts corresponding to objects that are part of, and, respectively,  have as part other objects. ). 
As a heuristic that ensures that they are relevant to the content of the text, we ensure that both their arguments are words that occur in the document, when connecting their corresponding synsets via WordNet relations. By constraining the two ends of an "is-a" or "part-of" edge to occur in the document, we avoid relations derived from  synsets unrelated to the document's content. In fact, this provides an  effective word-sense disambiguation heuristic.


The Prolog-based dialog engine
After our Python-based document processor, with help from the Stanford dependency parser, builds and ranks the text graph and extracts summaries, keyphrases and relations, we pass them to the Prolog-based  dialog engine.


Generating  input for post-processing by logic programs

Once the document is processed, we generate,
besides the dependency links provided by the parser,
relations containing  facts that we have gleaned 
from processing the document. Together, they form
a Prolog database representing the content of the document.

To keep the interface simple and portable to other logic programming tools, we generate the following predicates in the form of Prolog-readable code, in one file per document:
keyword(WordPhrase). -  the extracted keyphrases
summary(SentenceId,SentenceWords). -  the extracted summary sentences sentence identifiers and list of words
dep(SentenceID,WordFrom,FromTag,Label,WordTo,ToTag). - a component of a dependency link, with the first argument indicating the sentence they have been extracted
edge(SentenceID,FromLemma,FromTag,RelationLabel,ToLemma,ToTag). -  edge marked with sentence identifiers indicating  where it was extracted from, and the lemmas with their POS tags at the two ends of the edge

rank(LemmaOrSentenceId,Rank). - the rank computed for each lemma
w2l(Word,Lemma,Tag). - a map associating to each word a lemma, as found by the POS tagger
svo(Subject,Verb,Object,SentenceId). - subject-verb-object relations extracted from parser input or WordNet-based isa and partof labels in verb position 
sent(SentenceId,ListOfWords). - the list of sentences in the document with a sentence identifier as first argument and a list of words as second argument
They provide a relational view of a document in the form of a database that  will support  the inference mechanisms built on top of it.

The resulting logic program can then be processed  with Prolog semantics, possibly enhanced by using 
constraint solvers, 
abductive reasoners or via Answer Set Programming systems. Specifically, we expect benefits from such extensions for tackling computationally difficult problems like word-sense disambiguation (WSD) or entailment inference
as well as domain-specific reasoning.

We have applied this process to the Krapivin document set, a  collection of  2304 research papers annotated with the authors' own keyphrases and abstracts.
  
The resulting 3.5 GB Prolog 
dataset(http://www.cse.unt.edu/ tarau/datasets/PrologDeepRankDataset.zip) 
is made available for researchers in the field, interested to explore declarative reasoning or text mining mechanisms. 

The Prolog interface

We use as a logic processing tool the open 
source SWI-Prolog  system(http://www.swi-prolog.org/)
 that
 can be called from, and can call Python programs using
 the pyswip adaptor(https://github.com/yuce/pyswip).
 After the adaptor creates the Prolog process and the content of the 
 digested document
 is transferred from Python (in a few seconds for typical 
 scientific paper sizes of 10-15 pages), query processing is realtime.



The user interaction loop

With the Prolog representation of the digested document in memory, the dialog starts by displaying the summary and keyphrases extracted from the 
document(And also speak them out if the quiet flag is off.).
One can see this as a "mini search-engine", specialized to the document, and, with help of an indexing layer, extensible to multi-document collections. The dialog agent associated to the document answers queries as sets of salient sentences extracted from the text, via a specialization of our summarization algorithm to the context inferred from the query.

As part of an interactive  read/listen, evaluate, print/say loop,
we generate for each query sentence, a set of predicates that
are passed to the Prolog process, from where answers will come back
via the pyswip interface.
The predicates extracted from a query have the same structure as the 
database 
representing the content of the complete document,
initially sent to Prolog.

The answer generation algorithm

Answers are generated by selecting the most relevant sentences, presented
in their natural order in the text, in the form of a specialized "mini-summary".

Query expansion

Answer generation starts with a query-expansion mechanism
via relations that are derived by
finding, for lemmas in the query,
WordNet hypernyms, hyponyms, meronyms and holonyms,
as well as by directly extracting them 
from the query's dependency links.
We use the rankings available both in the query
and the document graph to prioritize the highest ranked sentences connected to the highest ranked  nodes in the query.

Short-time dialog memory

We keep  representations of recent queries in memory, as well as the
answers generated for them.
If the representation of the  current query overlaps with a past one, we use content in the past query's database to extend query expansion
to cover edges originating from that query.
Overlapping is detected via
shared edges between noun or verb nodes 
between the query graphs.






Sentence selection

Answer sentence selection happens by a combination of several
interoperating algorithms:
use of personalized PageRank with a dictionary provided by highest ranking lemmas and their ranks in the query's graph, followed by reranking the document's graph to specialize to the query's content
matching guided by SVO-relations
matching of edges in the query graph against edges in the document graph
query expansion guided by rankings in both the query graph and the document graph
matching guided by a selection of related content components in the short-term dialog memory window
Matching against the Prolog database representing the document
is currently implemented  as a size constraint on the 
intersection of the expanded
query lemma set, built with highly ranked shared lemmas pointing to sentences
containing them.
The set of answers is organized to return the highest-ranked sentences based 
on relevance to the query and in the order in which they appear in the document.

We keep the dialog window relatively small (limited to the highest ranked 3 sentences in the answer set, by default). 
Relevance is ensured with help from the rankings computed for both
the  document content and the query. 

Interacting with the dialog engine

The following example shows the result of a query on the US Constitution document.
removeGraph of query on the U.S. Constitution0.38remove


>>> talkabout('examples/const')

?-- How can a President be removed from office?

59 : In Case of the Removal of the President from Office , or of his Death , Resignation , or Inability to discharge the Powers and Duties of the said Office , the same shall devolve on the Vice President , and the Congress may by Law provide for the Case of Removal , Death , Resignation or Inability , both of the President and Vice President , declaring what Officer shall then act as President , and such Officer shall act accordingly , until the Disability be removed , or a President shall be elected . 

66 : Section 4 The President , Vice President and all civil Officers of the United States , shall be removed from Office on Impeachment for , and Conviction of , Treason , Bribery , or other high Crimes and Misdemeanors . 

190 : If the Congress , within twenty one days after receipt of the latter written declaration , or , if Congress is not in session , within twenty one days after Congress is required to assemble , determines by two thirds vote of both Houses that the President is unable to discharge the powers and duties of his office , the Vice President shall continue to discharge the same as Acting President ; otherwise , the President shall resume the powers and duties of his office .



Note the relevance of the extracted sentences and resilience to semantic and syntactic variations (e.g., the last sentence does not contain the word "remove").
The dependency graph of the query is shown in Fig. . The clauses of the queryrank/2 predicate in the Prolog database corresponding to the query are:

query_rank('President', 0.2162991696472837).
query_rank('remove', 0.20105324712764877).
query_rank('office', 0.12690425831428373).
query_rank('how', 0.04908035060099132).
query_rank('can', 0.04908035060099132).
query_rank('a', 0.04908035060099132).
query_rank('be', 0.04908035060099132).
query_rank('from', 0.04908035060099132).
query_rank(0, 0.0023633884483800784).


Our next example uses an ASCII version of
Einstein's 1920 book on relativity, retrieved from the Gutenberg
collection(
https://www.gutenberg.org/files/30155/30155-0.txt
)
and trimmed to the actual content of the book (250 pages in epub form).


>>> talkabout('examples/relativity')

?-- What happens to light in the presence of gravitational fields?

611 : In the example of the transmission of light just dealt with , we have seen that the general theory of relativity enables us to derive theoretically the influence of a gravitational field on the course of natural processes , the laws of which are already known when a gravitational field is absent . 

764 : On the contrary , we arrived at the result that according to this latter theory the velocity of light must always depend on the co-ordinates when a gravitational field is present . 

765 : In connection with a specific illustration in Section XXIII , we found that the presence of a gravitational field invalidates the definition of the coordinates and the time , which led us to our objective in the special theory of relativity . 


lightGraph of query on Einstein's book on Relativity0.30light

The query graph is shown in Fig. .
After the less than 30 seconds that it takes to digest the book, answers
are generated in less than a second for all queries that we have tried.

Given the availability of spoken dialog, a user can iterate and refine
queries to extract the most relevant answer sentences of a document.


On an even larger document, like the Tesla Model 3 
owner's manual(
https://www.tesla.com/sites/default/files/model_3_owners_manual_north_america_en.pdf
),
digesting the document takes about 60 seconds and results in 12 MB of Prolog clauses. After that, query answering is still below 1 second.

>>> talkabout('examples/tesla')

?-- How may I have a flat tire repaired?

3207 : Arrange to have Model 3 transported to a Tesla Service Center , or to a nearby tire repair center . 

3291 : Note : If a tire has been replaced or repaired using a different tire sealant than the one available from Tesla , and a low tire pressure is detected , it is possible that the tire sensor has been damaged . 

The highly relevant first answer is genuinely useful in this case, given that Tesla Model 3's do not have a spare tire. Being able to use voice queries while driving and in need of urgent technical information about one's car, hints towards obvious practical applications of our dialog engine.

Discussion
Ideally, one would like to evaluate the quality of natural language understanding of an AI system by querying it not only about a set of relations explicitly extracted in the text, but also about relations inferred from the text. Moreover, one would like also to have the system justify the inferred relations in the form of a proof, or at least a sketch of the thought process a human would use for the same purpose.
The main challenge here is not only that theorem-proving logic is hard, (with first-order classical predicate calculus already Turing-complete), but also that modalities, beliefs, sentiments, hypothetical and contrafactual judgements often make the underlying knowledge structure intractable.

On the other hand, simple relations, stated or implied by text elements that can be mined or inferred from a ranked graph built from labeled dependency links, provide a limited but manageable approximation of the text's deeper logic structure, especially when aggregated with generalizations and similarities  provided by WordNet or the much richer Wikipedia knowledge graph. 

Given its effectiveness as an interactive content exploration tool, we plan future work on packaging our dialog engine as a set of Amazon Alexa skills for some popular Wikipedia entries as well as  product reviews, FAQs and user manuals.

Empirical evaluation of our keyphrase and summarization algorithms will be subject to a different paper, but preliminary tests indicate that both of them match or exceed Rouge scores for state of the art systems.

Related workDependency parsing

The Stanford neural network based dependency parser  is now part of the Stanford CoreNLP toolkit(https://stanfordnlp.github.io/CoreNLP/), which also comes with  part of speech tagging, named entities recognition and co-reference resolution.
Its evolution toward the use of Universal Dependencies makes
tools relying on it potentially portable to
over 70  languages  covered by the Universal Dependencies effort
(https://universaldependencies.org/).
 
Of particular interest is the connection of dependency graphs  to logic elements like predicate argument relations. The mechanism of automatic conversion of constituency trees to dependency graphs described in provides a bridge allowing the output of high-quality statistically trained phrase structure parsers to be reused for extraction of dependency links.

We analyze
dependency links and POS-tags associated to their endpoints
to  extract SVO relations. By redirecting links to focus
on nouns and sentences we not only enable keyphrase and summary
extraction from the resulting document graph
but also facilitate its use for query answering 
in our dialog engine.


Graph based Natural Language Processing

In TextRank keyphrases are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a sliding window of 2 to 10 words.
Sentence similarity is computed as content overlap giving weights on the links that refine the original PageRank algorithm. TextRank needs elimination of stop words and reports best results when links are restricted to nouns and adjectives.
In several graph centrality measures are explored and offers a comprehensive overview on graph-based natural language processing and related graph algorithms.
Graph-based and other text summarization techniques are surveyed in and more recently in. Besides ranking, elements like coherence via similarity with previously chosen sentences
and avoidance of redundant rephrasings are shown to contribute to the overall quality of
the summaries.

Beyond summaries obtained by aggregating important sentences extracted from a document, and possibly applying to them sentence compression techniques that remove redundant or less relevant words, new techniques are emerging for abstractive summarization. 
For this purpose, in the context of graph-based processing, one clearly benefits from as much syntactic and semantic information as possible, given also the need to synthesize new sentences subject to syntactic and semantic constraints.

The main novelty of our approach in this context is building the text graph from dependency links and integrating words and sentences in the same text graph, resulting in a unified algorithm that also enables relation extraction and interactive text mining.




Relation Extraction

The relevance of dependency graphs for relation extraction has been identified in several papers, with pointing out to their role as a generic interface between parsers and relation extraction systems. 
In several  models grounded on syntactic patterns are identified (e.g., subject-verb-object) that can be mined out from dependency graphs.
Of particular interest for relation extraction facilitated by dependency graphs is the shortest path hypothesis that prefers relating entities like predicate arguments that are connected via a shortest paths in the graph.
To facilitate their practical applications to biomedical texts, 
 extends dependency graphs with focus on richer sets of semantic features including "is-a" and "part-of" relations and co-reference resolution.

The use of ranking algorithms in combination with WordNet synset links for word-sense disambiguation goes back as far as, in fact a prequel to the TextRank paper.
With the emergence of resources like Wikipedia, a much richer set of links and content elements has been used in connection with graph based natural language processing  
.

We currently extract our relations directly from the dependency graph
and by using one step up and one step down links in the WordNet hypernym and meronym hierarchies, but extensions are planned to integrate Wikipedia content, via the dbpedia database(https://wiki.dbpedia.org/) and to extract more elaborate logic relations using a Prolog-based semantic parser like Boxer.

Logic Programming Systems for Natural Language Processing

A common characteristic of Prolog or ASP-based NLP systems is their focus on closed domains with domain-specific logic expressed in clausal form,
although recent work like extracts action language programs from more general narratives.

As our main objective is the building of a practically useful dialog agent, and as we work with open domain text and query driven content retrieval, our focus is not on precise domain-specific reasoning mechanisms. By taking advantage of the Prolog representation of a document's content, we use reasoning about the extracted relations and ranking information to find the most  relevant sentences derived from a given query and the recent dialog history.


Conclusions

The key idea of the paper has evolved from our search for synergies between symbolic AI and emerging machine-learning based natural language processing tools. It is our belief that these are complementary and that by working together they will take significant forward steps in natural language understanding.
We have based our text graph on heterogeneous, but syntactically and semantically meaningful text units (words and sentences) resulting in a web of interleaved  links, mutually recommending each other's highly ranked instances. Our  fact extraction algorithm, in combination with the Prolog interface  has elevated the
syntactic information provided by dependency graphs with semantic elements
ready to benefit from logic-based inference mechanisms.
Given the standardization brought by the use of Universal Dependencies, 
our techniques are  likely to be portable to a large number of languages.

The Prolog-based dialog engine supports spoken interaction with a conversational agent that exposes salient content of the document driven by the user's interest. Its applications range from assistive technologies to visually challenged people, voice interaction with user manuals, teaching from K-12 to graduate level and interactive information retrieval from complex technical or legal documents.

Last but not least, we have used our system's front end to generate
 the  Prolog
dataset at http://www.cse.unt.edu/ tarau/datasets/PrologDeepRankDataset.zip,
derived from more than 2000 research papers and made it available to other researchers using logic programming based reasoners and content mining tools.

Acknowledgment
We are thankful to the anonymous reviewers of PADL'2020 for their careful reading and constructive suggestions.
